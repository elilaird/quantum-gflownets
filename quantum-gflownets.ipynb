{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update package resources to account for version changes.\n",
    "import importlib, pkg_resources\n",
    "importlib.reload(pkg_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-probability==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "\n",
    "import gym, cirq, sympy\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import deque, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_qubit_rotation(qubit, symbols):\n",
    "    \"\"\"\n",
    "    Returns Cirq gates that apply a rotation of the bloch sphere about the X,\n",
    "    Y and Z axis, specified by the values in `symbols`.\n",
    "    \"\"\"\n",
    "    return [cirq.rx(symbols[0])(qubit),\n",
    "            cirq.ry(symbols[1])(qubit),\n",
    "            cirq.rz(symbols[2])(qubit)]\n",
    "\n",
    "def entangling_layer(qubits):\n",
    "    \"\"\"\n",
    "    Returns a layer of CZ entangling gates on `qubits` (arranged in a circular topology).\n",
    "    \"\"\"\n",
    "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
    "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
    "    return cz_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circuit(qubits, n_layers):\n",
    "    \"\"\"Prepares a data re-uploading circuit on `qubits` with `n_layers` layers.\"\"\"\n",
    "    # Number of qubits\n",
    "    n_qubits = len(qubits)\n",
    "\n",
    "    # Sympy symbols for variational angles\n",
    "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
    "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
    "\n",
    "    # Sympy symbols for encoding angles\n",
    "    inputs = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_qubits})')\n",
    "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
    "\n",
    "    # Define circuit\n",
    "    circuit = cirq.Circuit()\n",
    "    for l in range(n_layers):\n",
    "        # Variational layer\n",
    "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
    "        circuit += entangling_layer(qubits)\n",
    "        # Encoding layer\n",
    "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
    "\n",
    "    # Last varitional layer\n",
    "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i,q in enumerate(qubits))\n",
    "\n",
    "    return circuit, list(params.flat), list(inputs.flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits, n_layers = 3, 1\n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
    "circuit, _, _ = generate_circuit(qubits, n_layers)\n",
    "SVGCircuit(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReUploadingPQC(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Performs the transformation (s_1, ..., s_d) -> (theta_1, ..., theta_N, lmbd[1][1]s_1, ..., lmbd[1][M]s_1,\n",
    "        ......., lmbd[d][1]s_d, ..., lmbd[d][M]s_d) for d=input_dim, N=theta_dim and M=n_layers.\n",
    "    An activation function from tf.keras.activations, specified by `activation` ('linear' by default) is\n",
    "        then applied to all lmbd[i][j]s_i.\n",
    "    All angles are finally permuted to follow the alphabetical order of their symbol names, as processed\n",
    "        by the ControlledPQC.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
    "        super(ReUploadingPQC, self).__init__(name=name)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = len(qubits)\n",
    "\n",
    "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
    "\n",
    "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
    "        self.theta = tf.Variable(\n",
    "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
    "            trainable=True, name=\"thetas\"\n",
    "        )\n",
    "\n",
    "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
    "        self.lmbd = tf.Variable(\n",
    "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
    "        )\n",
    "\n",
    "        self.logZ = tf.Variable(\n",
    "            initial_value=0.0, dtype=\"float32\", trainable=True, name=\"logZ\"\n",
    "        )\n",
    "\n",
    "        # Define explicit symbol order.\n",
    "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
    "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
    "\n",
    "        self.activation = activation\n",
    "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
    "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs[0] = encoding data for the state.\n",
    "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
    "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
    "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
    "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
    "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
    "\n",
    "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
    "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
    "\n",
    "        return self.computation_layer([tiled_up_circuits, joined_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4 # Dimension of the state vectors in CartPole\n",
    "n_layers = 5 # Number of layers in the PQC\n",
    "n_actions = 2 # Number of actions in CartPole\n",
    "\n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = [cirq.Z(q) for q in qubits]\n",
    "observables = [reduce((lambda x, y: x * y), ops)] # Z_0*Z_1*Z_2*Z_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQC-GFlowNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smiley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReUploadingPQC(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Performs the transformation (s_1, ..., s_d) -> (theta_1, ..., theta_N, lmbd[1][1]s_1, ..., lmbd[1][M]s_1,\n",
    "        ......., lmbd[d][1]s_d, ..., lmbd[d][M]s_d) for d=input_dim, N=theta_dim and M=n_layers.\n",
    "    An activation function from tf.keras.activations, specified by `activation` ('linear' by default) is\n",
    "        then applied to all lmbd[i][j]s_i.\n",
    "    All angles are finally permuted to follow the alphabetical order of their symbol names, as processed\n",
    "        by the ControlledPQC.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\", noise=False):\n",
    "        super(ReUploadingPQC, self).__init__(name=name)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = len(qubits)\n",
    "\n",
    "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
    "\n",
    "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
    "        self.theta = tf.Variable(\n",
    "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
    "            trainable=True, name=\"thetas\"\n",
    "        )\n",
    "\n",
    "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
    "        self.lmbd = tf.Variable(\n",
    "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
    "        )\n",
    "\n",
    "        self.logZ = tf.Variable(\n",
    "            initial_value=1.0, dtype=\"float32\", trainable=True, name=\"logZ\"\n",
    "        )\n",
    "\n",
    "        # Define explicit symbol order.\n",
    "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
    "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
    "\n",
    "        self.activation = activation\n",
    "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
    "        if not noise:\n",
    "            self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)\n",
    "        else:\n",
    "            self.computation_layer = tfq.layers.NoisyControlledPQC(circuit, observables, repetitions=10, sample_based=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs[0] = encoding data for the state.\n",
    "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
    "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
    "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
    "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
    "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
    "\n",
    "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
    "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
    "\n",
    "        return self.computation_layer([tiled_up_circuits, joined_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alternating(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, init_val=-1.0):\n",
    "        super(Alternating, self).__init__()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=tf.constant([[(init_val)**i for i in range(output_dim)]]), dtype=\"float32\",\n",
    "            trainable=True, name=\"obs-weights\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "base_face = lambda: (pp.gca().add_patch(pp.Circle((0.5,0.5),0.5,fc=(.9,.9,0))),\n",
    "                     pp.gca().add_patch(pp.Circle((0.25,0.6),0.1,fc=(0,0,0))),\n",
    "                     pp.gca().add_patch(pp.Circle((0.75,0.6),0.1,fc=(0,0,0))))\n",
    "patches = {\n",
    "  'smile': lambda: pp.gca().add_patch(pp.Polygon(np.stack([np.linspace(0.2,0.8), 0.3-np.sin(np.linspace(0,3.14))*0.15]).T, closed=False, fill=False, lw=3)),\n",
    "  'frown': lambda: pp.gca().add_patch(pp.Polygon(np.stack([np.linspace(0.2,0.8), 0.15+np.sin(np.linspace(0,3.14))*0.15]).T, closed=False, fill=False, lw=3)),\n",
    "  'left_eb_down': lambda: pp.gca().add_line(pp.Line2D([0.15, 0.35], [0.75,0.7], color=(0,0,0))),\n",
    "  'right_eb_down': lambda: pp.gca().add_line(pp.Line2D([0.65, 0.85], [0.7,0.75], color=(0,0,0))),\n",
    "  'left_eb_up': lambda: pp.gca().add_line(pp.Line2D([0.15, 0.35], [0.7,0.75], color=(0,0,0))),\n",
    "  'right_eb_up': lambda: pp.gca().add_line(pp.Line2D([0.65, 0.85], [0.75,0.7], color=(0,0,0))),\n",
    "}\n",
    "sorted_keys = sorted(patches.keys())\n",
    "\n",
    "def draw_face(face):\n",
    "  base_face()\n",
    "  for i in face:\n",
    "    patches[i]()\n",
    "  pp.axis('scaled')\n",
    "  pp.axis('off')\n",
    "\n",
    "f, ax = pp.subplots(1,2)\n",
    "pp.sca(ax[0])\n",
    "draw_face(['smile', 'left_eb_down', 'right_eb_down'])\n",
    "pp.sca(ax[1])\n",
    "draw_face(['frown', 'left_eb_up', 'right_eb_up'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smiley Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_overlap(face):\n",
    "  # Can't have two overlapping eyebrows!\n",
    "  if 'left_eb_down' in face and 'left_eb_up' in face:\n",
    "    return True\n",
    "  if 'right_eb_down' in face and 'right_eb_up' in face:\n",
    "    return True\n",
    "  # Can't have two overlapping mouths!\n",
    "  if 'smile' in face and 'frown' in face:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def face_reward(face):\n",
    "  if has_overlap(face):\n",
    "    return tf.constant(0.0)\n",
    "  eyebrows = 'left_eb_down', 'left_eb_up', 'right_eb_down', 'right_eb_up'\n",
    "  # Must have exactly two eyebrows\n",
    "  if sum([i in face for i in eyebrows]) != 2:\n",
    "    return tf.constant(0.0)\n",
    "  # We want twice as many happy faces as sad faces so here we give a reward of 2 for smiles\n",
    "  if 'smile' in face:\n",
    "    return tf.constant(4.0)\n",
    "  if 'frown' in face:\n",
    "    return tf.constant(2.0)  # and a reward of 1 for frowns\n",
    "  # If we reach this point, there's no mouth\n",
    "  return tf.constant(0.0)\n",
    "\n",
    "\n",
    "unsorted_keys = ['smile', 'frown', 'left_eb_down', 'right_eb_down', 'left_eb_up', 'right_eb_up']\n",
    "\n",
    "# We first define how the model will view a face, i.e. how to encode a face in\n",
    "# a tensor\n",
    "def face_to_tensor(face):\n",
    "  return tf.convert_to_tensor([tf.constant([float(i in face) for i in unsorted_keys], dtype=tf.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionMasking(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_size, mask_value=-100, pass_through=False):\n",
    "        super(ActionMasking, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.mask_value = mask_value\n",
    "        self.pass_through = pass_through\n",
    "\n",
    "    def call(self, logits, x):\n",
    "        indices_f = tf.range(self.state_size)\n",
    "        indices_b = tf.range(self.state_size, self.state_size * 2)\n",
    "        \n",
    "        if not self.pass_through:\n",
    "            P_f = tf.gather(logits, indices_f, axis=1) * (1 - x) + x * self.mask_value\n",
    "            P_b = tf.gather(logits, indices_b, axis=1) * x + (1 - x) * self.mask_value\n",
    "        else:\n",
    "            P_f = tf.gather(logits, indices_f, axis=1)\n",
    "            P_b = tf.gather(logits, indices_b, axis=1)\n",
    "            \n",
    "        return P_f, P_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 6 # Dimension of the state vectors in Smiley\n",
    "n_layers = 5 # Number of layers in the PQC\n",
    "n_actions = 6*2 # Number of actions in Smiley\n",
    "\n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
    "\n",
    "ops = [cirq.Z(q) for q in qubits]\n",
    "observables = [reduce((lambda x, y: x * y), ops)] # Z_0*Z_1*Z_2*Z_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit, _, _ = generate_circuit(qubits, n_layers)\n",
    "SVGCircuit(circuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forward_flow(qubits, n_layers, n_actions, beta, observables, noise=False):\n",
    "    \"\"\"Generates a Keras model for a data re-uploading PQC Gflownet.\"\"\"\n",
    "\n",
    "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
    "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables, noise=noise)([input_tensor])\n",
    "    alternating = Alternating(n_actions)(re_uploading_pqc)\n",
    "    lmbd1 = tf.keras.layers.Lambda(lambda x: x * beta)(alternating)\n",
    "    action_masking = ActionMasking(n_qubits, mask_value=-100)(lmbd1, input_tensor)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_tensor], outputs=action_masking)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = generate_forward_flow(qubits, n_layers, n_actions, 1.0, observables, noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQC-Gflownet Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = ['theta','lambda', 'logZ', 'w_out']\n",
    "# print number of parameters in the model\n",
    "print(f\"Number of parameters in the model: {model.count_params()}\")\n",
    "for i in range(4):\n",
    "    print(f\"Parameter {param_names[i]}: {model.trainable_variables[i].numpy().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_forward_flow(qubits, n_layers, n_actions, 1.0, observables, noise=True)\n",
    "\n",
    "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=False)\n",
    "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=False)\n",
    "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=False)\n",
    "optimizer_logz = tf.keras.optimizers.Adam(learning_rate=0.0001, amsgrad=False)\n",
    "\n",
    "# Assign the model parameters to each optimizer\n",
    "w_in, w_var, w_out, w_logz = 1, 0, 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "\n",
    "Categorical = tfp.distributions.Categorical\n",
    "\n",
    "NUM_EPISODES = 50000\n",
    "REWARD_SCALE = 1.0\n",
    "update_freq = 10\n",
    "minibatch_loss = 0.0\n",
    "all_actions = []\n",
    "\n",
    "sampled_faces = []\n",
    "losses = []\n",
    "grads_list = []\n",
    "logZs = []\n",
    "ep_times = []\n",
    "tot_times = []\n",
    "\n",
    "for episode in (pbar := tqdm(range(NUM_EPISODES))):\n",
    "    state = []\n",
    "    \n",
    "    total_forward_flow = 0.0\n",
    "    total_backward_flow = 0.0\n",
    "    start_time = time.time()\n",
    "    start_time2 = time.time()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "\n",
    "        for t in range(3):\n",
    "\n",
    "            # compute flows \n",
    "            p_f, p_b = model(face_to_tensor(state))\n",
    "\n",
    "            cat = Categorical(logits=p_f[0])\n",
    "            action = cat.sample().numpy()\n",
    "            all_actions.append(action)\n",
    "            # take action\n",
    "            new_state = state + [unsorted_keys[action]]\n",
    "\n",
    "            # accumulate forward flow\n",
    "            total_forward_flow += cat.log_prob(action)\n",
    "\n",
    "            if t == 2:\n",
    "                reward = face_reward(new_state) * REWARD_SCALE\n",
    "\n",
    "            # recompute flows for new state\n",
    "            p_f, p_b = model(face_to_tensor(new_state))\n",
    "\n",
    "            # accumulate backward flow\n",
    "            total_backward_flow += Categorical(logits=p_b[0]).log_prob(action)\n",
    "\n",
    "            # update state\n",
    "            state = new_state\n",
    "\n",
    "        # compute loss\n",
    "        loss = tf.square(model.trainable_variables[w_logz] + total_forward_flow - tf.clip_by_value(tf.math.log(reward), -20, 10) - total_backward_flow)\n",
    "        #loss = tf.square(np.log(12.0) + total_forward_flow - tf.clip_by_value(tf.math.log(reward), -20, 10) - total_backward_flow)\n",
    "        minibatch_loss += loss\n",
    "\n",
    "    ep_times.append(time.time() - start_time)\n",
    "\n",
    "    sampled_faces.append(state)\n",
    "    if episode % update_freq == 0:\n",
    "        # gradient update\n",
    "        grads = tape.gradient(minibatch_loss, model.trainable_variables)\n",
    "        for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out, optimizer_logz], [w_in, w_var, w_out, w_logz]):\n",
    "            optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])\n",
    "        \n",
    "        grads_list.append(grads)\n",
    "        losses.append(minibatch_loss.numpy())\n",
    "        logZs.append(model.trainable_variables[w_logz].numpy())\n",
    "        minibatch_loss = 0.0\n",
    "    \n",
    "    tot_times.append(time.time() - start_time2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained weights\n",
    "model.save_weights('qgflownet-frown-noisy_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average episode time: \", np.mean(ep_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average total time: \", np.mean(tot_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.exp(logZs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f, ax = pp.subplots(2, 1, figsize=(10,6))\n",
    "pp.sca(ax[0])\n",
    "pp.plot(losses)\n",
    "pp.yscale('log')\n",
    "pp.ylabel('loss')\n",
    "pp.sca(ax[1])\n",
    "pp.plot(np.exp(logZs))\n",
    "pp.ylabel('estimated Z');\n",
    "\n",
    "# save figure\n",
    "pp.savefig('qgflownet-frown-noisy_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = pp.subplots(8,8,figsize=(4,4))\n",
    "print('Ratio of faces with a smile:', sum(['smile' in i for i in sampled_faces[-100:]]) / 100)\n",
    "print('Ratio of valid faces:', sum([face_reward(i).numpy() > 0 for i in sampled_faces[-100:]]) / 100)\n",
    "for i, face in enumerate(sampled_faces[-64:]):\n",
    "  pp.sca(ax[i//8,i%8])\n",
    "  draw_face(face)\n",
    "\n",
    "# save figure\n",
    "pp.savefig('qgflownet-frown-noisy_faces.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(['frown' in i for i in sampled_faces[-100:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, N, keys=None):\n",
    "    if keys is None:\n",
    "        keys = ['smile', 'frown', 'left_eb_down', 'right_eb_down', 'left_eb_up', 'right_eb_up']\n",
    "    sampled_faces = []\n",
    "    for _ in range(N):\n",
    "        state = []\n",
    "        for _ in range(3):\n",
    "            p_f, _ = model(face_to_tensor(state))\n",
    "            action = Categorical(logits=p_f[0]).sample().numpy()\n",
    "            new_state = state + [keys[action]]\n",
    "            state = new_state\n",
    "        sampled_faces.append(state)\n",
    "    return sampled_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(model, 100)\n",
    "print('Ratio of faces with a smile:', sum(['smile' in i for i in samples]) / 100)\n",
    "print('Ratio of valid faces:', sum([face_reward(i).numpy() > 0 for i in samples]) / 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
