{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "\n",
    "\n",
    "base_face = lambda: (\n",
    "    pp.gca().add_patch(pp.Circle((0.5, 0.5), 0.5, fc=(0.9, 0.9, 0))),\n",
    "    pp.gca().add_patch(pp.Circle((0.25, 0.6), 0.1, fc=(0, 0, 0))),\n",
    "    pp.gca().add_patch(pp.Circle((0.75, 0.6), 0.1, fc=(0, 0, 0))),\n",
    ")\n",
    "patches = {\n",
    "    \"smile\": lambda: pp.gca().add_patch(\n",
    "        pp.Polygon(\n",
    "            np.stack(\n",
    "                [\n",
    "                    np.linspace(0.2, 0.8),\n",
    "                    0.3 - np.sin(np.linspace(0, 3.14)) * 0.15,\n",
    "                ]\n",
    "            ).T,\n",
    "            closed=False,\n",
    "            fill=False,\n",
    "            lw=3,\n",
    "        )\n",
    "    ),\n",
    "    \"frown\": lambda: pp.gca().add_patch(\n",
    "        pp.Polygon(\n",
    "            np.stack(\n",
    "                [\n",
    "                    np.linspace(0.2, 0.8),\n",
    "                    0.15 + np.sin(np.linspace(0, 3.14)) * 0.15,\n",
    "                ]\n",
    "            ).T,\n",
    "            closed=False,\n",
    "            fill=False,\n",
    "            lw=3,\n",
    "        )\n",
    "    ),\n",
    "    \"left_eb_down\": lambda: pp.gca().add_line(\n",
    "        pp.Line2D([0.15, 0.35], [0.75, 0.7], color=(0, 0, 0))\n",
    "    ),\n",
    "    \"right_eb_down\": lambda: pp.gca().add_line(\n",
    "        pp.Line2D([0.65, 0.85], [0.7, 0.75], color=(0, 0, 0))\n",
    "    ),\n",
    "    \"left_eb_up\": lambda: pp.gca().add_line(\n",
    "        pp.Line2D([0.15, 0.35], [0.7, 0.75], color=(0, 0, 0))\n",
    "    ),\n",
    "    \"right_eb_up\": lambda: pp.gca().add_line(\n",
    "        pp.Line2D([0.65, 0.85], [0.75, 0.7], color=(0, 0, 0))\n",
    "    ),\n",
    "}\n",
    "sorted_keys = sorted(patches.keys())\n",
    "\n",
    "\n",
    "def draw_face(face):\n",
    "    base_face()\n",
    "    for i in face:\n",
    "        patches[i]()\n",
    "    pp.axis(\"scaled\")\n",
    "    pp.axis(\"off\")\n",
    "\n",
    "\n",
    "f, ax = pp.subplots(1, 2)\n",
    "pp.sca(ax[0])\n",
    "draw_face([\"smile\", \"left_eb_down\", \"right_eb_down\"])\n",
    "pp.sca(ax[1])\n",
    "draw_face([\"frown\", \"left_eb_up\", \"right_eb_up\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_overlap(face):\n",
    "    # Can't have two overlapping eyebrows!\n",
    "    if \"left_eb_down\" in face and \"left_eb_up\" in face:\n",
    "        return True\n",
    "    if \"right_eb_down\" in face and \"right_eb_up\" in face:\n",
    "        return True\n",
    "    # Can't have two overlapping mouths!\n",
    "    if \"smile\" in face and \"frown\" in face:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def face_reward(face):\n",
    "    if has_overlap(face):\n",
    "        return 0\n",
    "    eyebrows = \"left_eb_down\", \"left_eb_up\", \"right_eb_down\", \"right_eb_up\"\n",
    "    # Must have exactly two eyebrows\n",
    "    if sum([i in face for i in eyebrows]) != 2:\n",
    "        return 0\n",
    "    # We want twice as many happy faces as sad faces so here we give a reward of 2 for smiles\n",
    "    if \"smile\" in face:\n",
    "        return 4\n",
    "    if \"frown\" in face:\n",
    "        return 2  # and a reward of 1 for frowns\n",
    "    # If we reach this point, there's no mouth\n",
    "    return 0\n",
    "\n",
    "\n",
    "def face_to_tensor(face):\n",
    "    return torch.tensor([i in face for i in sorted_keys]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TBModel(nn.Module):\n",
    "    def __init__(self, num_hid):\n",
    "        super().__init__()\n",
    "        # The input dimension is 6 for the 6 patches.\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(6, num_hid),\n",
    "            nn.LeakyReLU(),\n",
    "            # We now output 12 numbers, 6 for P_F and 6 for P_B\n",
    "            nn.Linear(num_hid, 12),\n",
    "        )\n",
    "        # log Z is just a single number\n",
    "        self.logZ = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.mlp(x)\n",
    "        # Slice the logits, and mask invalid actions (since we're predicting\n",
    "        # log-values), we use -100 since exp(-100) is tiny, but we don't want -inf)\n",
    "        P_F = logits[..., :6] * (1 - x) + x * -100\n",
    "        P_B = logits[..., 6:] * x + (1 - x) * -100\n",
    "        return P_F, P_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "model = TBModel(512)\n",
    "opt = torch.optim.Adam(model.parameters(), 3e-4)\n",
    "\n",
    "# Let's keep track of the losses and the faces we sample\n",
    "tb_losses = []\n",
    "tb_sampled_faces = []\n",
    "# To not complicate the code, I'll just accumulate losses here and take a\n",
    "# gradient step every `update_freq` episode.\n",
    "minibatch_loss = 0\n",
    "update_freq = 2\n",
    "ep_times = []\n",
    "\n",
    "logZs = []\n",
    "for episode in tqdm.tqdm(range(50000), ncols=40):\n",
    "    # Each episode starts with an \"empty state\"\n",
    "    state = []\n",
    "    # Predict P_F, P_B\n",
    "    P_F_s, P_B_s = model(face_to_tensor(state))\n",
    "    total_P_F = 0\n",
    "    total_P_B = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for t in range(3):\n",
    "        # Here P_F is logits, so we want the Categorical to compute the softmax for us\n",
    "        cat = Categorical(logits=P_F_s)\n",
    "        action = cat.sample()\n",
    "        # \"Go\" to the next state\n",
    "        new_state = state + [sorted_keys[action]]\n",
    "        # Accumulate the P_F sum\n",
    "        total_P_F += cat.log_prob(action)\n",
    "\n",
    "        if t == 2:\n",
    "            # If we've built a complete face, we're done, so the reward is > 0\n",
    "            # (unless the face is invalid)\n",
    "            reward = torch.tensor(face_reward(new_state)).float()\n",
    "        # We recompute P_F and P_B for new_state\n",
    "        P_F_s, P_B_s = model(face_to_tensor(new_state))\n",
    "        # Here we accumulate P_B, going backwards from `new_state`. We're also just\n",
    "        # going to use opposite semantics for the backward policy. I.e., for P_F action\n",
    "        # `i` just added the face part `i`, for P_B we'll assume action `i` removes\n",
    "        # face part `i`, this way we can just keep the same indices.\n",
    "        total_P_B += Categorical(logits=P_B_s).log_prob(action)\n",
    "\n",
    "        # Continue iterating\n",
    "        state = new_state\n",
    "\n",
    "    ep_times.append(time.time() - start_time)\n",
    "    # We're done with the trajectory, let's compute its loss. Since the reward can\n",
    "    # sometimes be zero, instead of log(0) we'll clip the log-reward to -20.\n",
    "    loss = (\n",
    "        model.logZ + total_P_F - torch.log(reward).clip(-20) - total_P_B\n",
    "    ).pow(2)\n",
    "    minibatch_loss += loss\n",
    "\n",
    "    # Add the face to the list, and if we are at an\n",
    "    # update episode, take a gradient step.\n",
    "    tb_sampled_faces.append(state)\n",
    "    if episode % update_freq == 0:\n",
    "        tb_losses.append(minibatch_loss.item())\n",
    "        minibatch_loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        minibatch_loss = 0\n",
    "        logZs.append(model.logZ.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "for p in model_parameters:\n",
    "    print(p.size())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model_parameters:\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.exp(logZs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take every 5th loss value\n",
    "losses = tb_losses[::5]\n",
    "zs = logZs[::5]\n",
    "\n",
    "f, ax = pp.subplots(2, 1, figsize=(10, 6))\n",
    "pp.sca(ax[0])\n",
    "pp.plot(losses)\n",
    "pp.yscale(\"log\")\n",
    "pp.ylabel(\"loss\")\n",
    "pp.sca(ax[1])\n",
    "pp.plot(np.exp(zs))\n",
    "pp.ylabel(\"estimated Z\")\n",
    "\n",
    "# save figure\n",
    "pp.savefig(\"./classical-training-loss-smile.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average episode time: \", np.mean(ep_times), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = pp.subplots(8, 8, figsize=(4, 4))\n",
    "print(\n",
    "    \"Ratio of faces with a smile:\",\n",
    "    sum([\"smile\" in i for i in tb_sampled_faces[-100:]]) / 100,\n",
    ")\n",
    "print(\n",
    "    \"Ratio of valid faces:\",\n",
    "    sum([face_reward(i) > 0 for i in tb_sampled_faces[-100:]]) / 100,\n",
    ")\n",
    "for i, face in enumerate(tb_sampled_faces[-64:]):\n",
    "    pp.sca(ax[i // 8, i % 8])\n",
    "    draw_face(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
